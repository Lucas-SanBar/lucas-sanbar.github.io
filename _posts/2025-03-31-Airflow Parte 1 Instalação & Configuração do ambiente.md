---
title: "Desbravando o Airflow - Parte 1 - Configurando ambiente e instala√ß√£o"
description: Aprenda a configura√ß√£o de um ambiente m√≠nimo necess√°rio para instala√ß√£o do Apache Airflow.
date: 2025-03-31
categories: [Pipeline de Dados, Apache Airflow]
tags: [airflow, dag, aws, ec2]
image: 
  path: /assets/images/31-03-2025/Capa-31-03-2025.png
---

## Introdu√ß√£o

Com o crescimento exponencial dos dados, a necessidade de process√°-los de forma eficiente nunca foi t√£o importante. Empresas e profissionais de dados enfrentam desafios constantes para extrair informa√ß√µes a partir de grandes volumes de dados armazenados em bancos relacionais, planilhas e outros formatos.

Neste (E nos pr√≥ximos) artigo, vamos explorar a constru√ß√£o de um pipeline de dados, come√ßando pela configura√ß√£o de uma inst√¢ncia ec2 e instala√ß√£o do Apache Airflow.

Vamos entender como essa ferramenta se complementa com outras para cria√ß√£o um fluxo de dados robusto e para garantir que tudo funcione de forma autom√°tica e escal√°vel, utilizaremos o Apache **Airflow** para orquestrar o pipeline, agendar execu√ß√µes e monitorar poss√≠veis falhas.

Ao final destes posts, voc√™ ter√° um pipeline funcional que pode ser adaptado para diversos cen√°rios, seja para processar dados de vendas, logs de sistemas ou qualquer outro conjunto de informa√ß√µes estruturadas.

## O que √© o Apache Airflow?

O Apache **Airflow** √© uma ferramenta de c√≥digo aberto para orquestra√ß√£o de fluxos de trabalho. Ele permite criar, agendar e monitorar pipelines de dados de maneira program√°vel e escal√°vel. O **Airflow** √© amplamente utilizado para automa√ß√£o de processos ETL, movimenta√ß√£o de dados entre sistemas e integra√ß√£o de ferramentas em um fluxo unificado.

![Intro 1](/assets/images/31-03-2025/Introdu√ß√£o 1.png)
_`Apache Airflow` Logo_

### Principais caracter√≠sticas

1. Baseado em DAGs (Directed Acyclic Graphs) ‚Äì Fluxos de trabalho s√£o representados como grafos direcionais ac√≠clicos, onde cada n√≥ representa uma tarefa/atividade a ser executada.
2. Possui interface amig√°vel e permite agendar tarefas e visualizar execu√ß√µes via interface web.
3. Suporte a operadores personalizados e integra√ß√£o com diversos servi√ßos como AWS, Google Cloud, Databricks e outros.
4. Pode ser escalado horizontalmente, distribuindo tarefas entre m√∫ltiplos workers.
5. Tarefas e pipelines s√£o definidos em c√≥digo Python, garantindo flexibilidade e controle.

![Intro 2](/assets/images/31-03-2025/Introdu√ß√£o 2.png)
_`Apache Airflow` Model_

### Casos de uso

1. **ETL/ELT** ‚Äì Extra√ß√£o, transforma√ß√£o e carga de dados.
2. **Orquestra√ß√£o de Machine Learning** ‚Äì Automa√ß√£o de pipelines de treinamento e infer√™ncia.
3. **Integra√ß√£o de Dados** ‚Äì Movimenta√ß√£o de dados entre bancos SQL, NoSQL, APIs e sistemas de armazenamento.
4. **Automa√ß√£o de Processos** ‚Äì Gatilhos e depend√™ncias entre sistemas empresariais.

## O que √© o AWS EC2?

O AWS EC2 (Elastic Compute Cloud) √© um servi√ßo da Amazon Web Services (AWS) que fornece m√°quinas virtuais na nuvem. Ele permite que voc√™ crie e gerencie servidores virtuais (chamados de inst√¢ncias) sob demanda, pagando apenas pelo tempo de uso.

![Intro 3](/assets/images/31-03-2025/Introdu√ß√£o 3.png)
_`Amazon EC2` Logo_

### Principais caracter√≠sticas

1. **Escalabilidade** ‚Äì Voc√™ pode aumentar ou diminuir a capacidade conforme necess√°rio.
2. **Modelos de Inst√¢ncia** ‚Äì Oferece inst√¢ncias otimizadas para diferentes cargas de trabalho (CPU, mem√≥ria, armazenamento, GPU, etc.).
3. **Op√ß√µes de Pre√ßo** ‚Äì Inclui On-Demand (pagamento por hora/segundo), Reserved Instances (contratos de longo prazo mais baratos), Spot Instances (descontos em servidores ociosos) e Savings Plans.
4. **Seguran√ßa** ‚Äì Integra√ß√£o com VPC (Virtual Private Cloud), IAM (Identity and Access Management) e outras ferramentas de seguran√ßa.
5. **Flexibilidade** de SO ‚Äì Suporte a Linux, Windows e outros sistemas operacionais.

![Intro 4](/assets/images/31-03-2025/Introdu√ß√£o 4.png)
_Sistemas operacionais suportados_

### Casos de uso

1. Hospedagem de aplica√ß√µes e sites.
2. Execu√ß√£o de bancos de dados e sistemas distribu√≠dos.
3. Treinamento de modelos de Machine Learning.
4. Simula√ß√µes e processamento de dados intensivos.
5. Desenvolvimento e testes de software.

## Apache Airflow + AWS EC2

Usar **AWS EC2** para hospedar o **Apache Airflow** √© uma abordagem comum para quem deseja ter controle total sobre a configura√ß√£o do Airflow, sem depender de solu√ß√µes gerenciadas mais robustas.

Essa abordagem funciona bem para projetos **pequenos e m√©dios**. Para workloads mais complexos, voc√™ pode considerar o MWAA (Managed Workflows for Apache Airflow) para evitar gerenciar a infraestrutura manualmente.

### Arquitetura B√°sica

![Intro 5](/assets/images/31-03-2025/Introdu√ß√£o 5.png)
_Apache Airflow + AWS EC2_

Para esta etapa utilizaremos a seguinte arquitetura e seguiremos os seguintes passos:

1. **Inst√¢ncia EC2** ‚Äì Cria√ß√£o e configura√ß√£o de uma inst√¢ncia EC2 (Ubunto Pro 22.04).
2. **Instala√ß√£o Apache Airflow** ‚Äì Instala√ß√£o e configura√ß√£o do Apache Airflow na inst√¢ncia criada na etapa anterior.
3. **Metadados** ‚Äì O Airflow precisa de um banco relacional para armazenar metadados (Usaremos PostgreSQL).
4. **Executores** ‚Äì Configura√ß√£o do executor local.
5. **Scheduler e Webserver** ‚Äì O Airflow precisa de um agendador para gerenciar tarefas e de um webserver para a interface gr√°fica.

## Criando a inst√¢ncia EC2

Vamos passar pelo passo a passo para criar uma inst√¢ncia EC2, configurar suas principais defini√ß√µes e acess√°-la remotamente.

### 1. Acessando o servi√ßo EC2

Ap√≥s acessar o console da AWS, caso seja a sua primeira vez fazendo a visita busque por `EC2` na caixa de pesquisa do console.

![EC2 1](/assets/images/31-03-2025/Image EC2 1.png)
_Caixa de pesquisa do console AWS_

Caso n√£o seja a sua primeira vez visitando as inst√¢ncias da AWS, voc√™ pode encontrar o servi√ßo em **Visitado Recentemente**

![EC2 2](/assets/images/31-03-2025/Image EC2 2.png)
_Visitado Recentemente do console AWS_

No painel da Amazon EC2, o primeiro passo √© executar uma inst√¢ncia, selecionando o seguinte bot√£o na home page do painel

![EC2 3](/assets/images/31-03-2025/Image EC2 3.png)
_Painel Amazon EC2_

### 2. Configurando a inst√¢ncia

![Image 1](/assets/images/31-03-2025/Image 1.png)
_Configura√ß√µes iniciais_

**1.** Defina um nome para sua inst√¢ncia. No meu caso a inst√¢ncia ser√° "Airflow_Server".

**2.** Defina o Sistema Operacional a ser utilizado na sua inst√¢ncia. No nosso caso selecione o **Ubunto Pro**.

>A melhor op√ß√£o atualmente √© o Ubuntu Pro, por conter compatibilidade total com o Apache Airflow, Python, Docker e PostgreSQL. Para mais informa√ß√µes acesse a documenta√ß√£o do Apache Airflow pelo seguinte [üîóLink](https://airflow.apache.org/docs/apache-airflow/stable/installation/prerequisites.html).
{: .prompt-warning }

**3.** Definina a imagem do Sistema Operacional para 22.04 LTS.

>A melhor op√ß√£o de imagem atualmete √© a 22.04 LTS, por conter maior suporte e estabilidade. A imagem 24.04 por ser um lan√ßamento mais recente pode conter instabilidades.
{: .prompt-warning }

![Image 2](/assets/images/31-03-2025/Image 2.png)
_Tipo da inst√¢ncia_

**4.** Selecione o tipo da inst√¢ncia para T2.Small ou maior.

>Para uma boa utiliza√ß√£o do Apache Airflow na sua inst√¢ncia o aconselhado √© no m√≠nimo 2GiB de mem√≥ria. Fique atento, qualquer tipo maior ou igual ao T2.Small pode ser **COBRADO**, para mais inforam√ß√£o acesse a calculadora de pre√ßos da AWS no seguinte [üîóLink](https://calculator.aws/#/).
{: .prompt-warning }

![Image 3](/assets/images/31-03-2025/Image 3.png)
_Par de chaves_

**5.** Crie um novo par de chaves para acesso a sua inst√¢ncia.
  - Selecione o tipo de par de chave como `RSA`;
  - Selecione o formato da chave como `.pem`;
  - Clique em `Criar par de chaves` ;

![Image 4](/assets/images/31-03-2025/Image 4.png)
_Configura√ß√£o da rede_

**6.** Para o nosso uso, ative:
  - Permiss√£o para `SSH`;
  - Permiss√£o para `HTTPs`;
  - Permiss√£o para `HTTP`;

>Para o nosso objetivo/case de uso essa configura√ß√£o j√° √© o suficiente. Mas saiba que existe outros meios de acesso mais seguros para produ√ß√£o, para mais informa√ß√µes acesse a documenta√ß√£o do Apache Airflow pelo seguinte [üîóLink](https://airflow.apache.org/docs/apache-airflow/stable/installation/prerequisites.html).
{: .prompt-warning }

![Image 4.1](/assets/images/31-03-2025/Image 4-1.png)
_Resumo Inst√¢ncia_

**7.** Confira o resumo da inst√¢ncia a ser criada e caso tudo esteja conforme seu desejo selecione `Executar inst√¢ncia`.

![Image 5](/assets/images/31-03-2025/Image 5.png)
_Inst√¢ncia criada_

**8.** Aguarde a cria√ß√£o da inst√¢ncia.

![Image 6](/assets/images/31-03-2025/Image 9.png)
_Aba Seguran√ßa_

![Image 7](/assets/images/31-03-2025/Image 10.png)
_Editar regras_

![Image 8](/assets/images/31-03-2025/Image 11.png)
_Criar nova regra_

**9.** Selecione a aba `Seguran√ßa` da inst√¢ncia criada.
  - V√° em `Editar regras de entrada`;
  - V√° em `Adicionar regra`;
  - Crie uma regra com as seguintes especifica√ß√µes (TCP Personalizado e porta 8080);

![Image 9](/assets/images/31-03-2025/Image 12.png)
_Regra criada_

**10.** Pronto, a inst√¢ncia foi configugurada e criada. Para acessar, basta selecionar a inst√¢ncia e ir em `Conectar`.


## Instalando o Apache Airflow

![Image EC2 4](/assets/images/31-03-2025/Image EC2 4.png)
_Acesso_

Ap√≥s acessar a inst√¢ncia criada nos passos anteriores, rode os seguintes comandos na ordem.

**1.** Atualize a lista de pacotes dispon√≠veis.
```
sudo apt update
```

**2.** Instale o gerenciador de pacotes `pip` para python 3.
```
sudo apt install python3-pip
```

**3.** Instale o SQLite3.
```
sudo apt install sqlite3
```

**4.** Instale o m√≥dulo `venv` para criar ambientes virtuais no Python `3.10`.
```
sudo apt install python3.10-venv
```

**5.** Crie um ambiente virtual chamao `venv` (Ou qualquer outro nome que desejar) para isolar as depend√™ncias no Python.
```
python3 -m venv venv
```

**6.** Ativa o ambiente virtual criado no passo anterior `venv`, permitindo o uso de pacotes intalados dentro dele.
```
source venv/bin/activate
```

**7.** Instale a biblioteca de desenvolvimento do PostgreSQL (Opcional se n√£o for utilizar PostgreSQL como banco para os metadados, necess√°rio para compilar extens√µes que interagem com o banco de dados PostgreSQL).
```
sudo apt-get install libpq-dev
```

**8.** Instala o Apache Airflow na vers√£o `2.5.0` com suporte ao PostgreSQL, garantindo compatibilidade com as depend√™ncias especificadas no arquivo para Python `3.7`.
```
pip install "apache-airflow[postgres]==2.5.0" --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-2.5.0/constraints-3.7.txt"
```

>Outras depend√™ncias podem ser utilizadas, verificar instala√ß√£o pelo [üîóLink](https://airflow.apache.org/docs/apache-airflow/stable/installation/installing-from-pypi.html).
{: .prompt-warning }

**9.** Inicialize o banco de daddos do Apache Airflow, criando as tabelas necess√°rias para o funcionamento do sistema.
```
airflow db init
```

**10.**  Instale o PostgreSQL e pacotes adicionais com funcionalidades extras para o banco de dados.
```
sudo apt-get install postgresql postgresql-contrib
```

**11.** Mude para o usu√°rio do sistema postgres, permitindo executar comandos administrativos no PostgreSQL.
```
sudo -i -u postgres
```

**12.** Abra o terminal interativo do PostgreSQL.
```
psql
```

**13.** Crie um banco de dados e um usu√°rio para o Airflow no PostgreSQL e garanta todos os privil√©gios sobre o banco criado.
```
CREATE DATABASE airflow;
CREATE USER airflow WITH PASSWORD 'airflow';
GRANT ALL PRIVILEGES ON DATABASE airflow TO airflow;
```

**14.** Saia do terminal do PosgreSQL utilizando `Ctrl + D` e `Ctrl + D` (2x).

**15.** Entre no diret√≥rio criado `airflow`.
```
cd airflow
```

**16.** Substitua a string de conex√£o no arquivo `airflow.cfg` por PostgreSQL ao inv√©s de SQLite.
```
sed -i 's#sqlite:////home/ubuntu/airflow/airflow.db#postgresql+psycopg2://airflow:airflow@localhost/airflow#g' airflow.cfg
```

**17.** Verifique se a string de conex√£o foi alterada.
```
grep sql_alchemy airflow.cfg
```

![Image 10](/assets/images/31-03-2025/Image 6.png)
_Verifique se a conex√£o foi alterada para PostgreSQL_

**18.** Verifique se o tipo de executor do Airflow √© do tipo `SequentialExecutor`.
```
grep executor airflow.cfg
```

![Image 11](/assets/images/31-03-2025/Image 7.png)
_Verifique o tipo do executor_

**19.** Substitua o m√©todo do executor para `LocalExecutor`.
```
sed -i 's#SequentialExecutor#LocalExecutor#g' airflow.cfg
```

**20.** Verifique se o tipo de executor foi alterado.
```
grep executor airflow.cfg
```

![Image 12](/assets/images/31-03-2025/Image 8.png)
_Verifique o tipo do executor foi alterado_

**21.** Crie o diret√≥rio para armazenamento dos DAGS.
```
mkdir dags
```

**22.** Inicie novamente o banco de dados do Airflow.
```
airflow db init
```

**23.** Crie um usu√°rio do Airflow.
  - Login: `airflow`
  - Senha: `airflow`

```
airflow users create -u airflow -f airflow -l airflow -r Admin -e airflow@gmail.com
```

**24.** Inicie o servidor web para interface gr√°fica.
```
airflow webserver &
```

**25.** Inicie o agendador do Airflow.
```
airflow scheduler
```

Se na execu√ß√£o de todos os comandos nenhum erro foi retornado, o apache foi instalado e configurado para acesso via navegador. Basta copiar o IPv4 DNS da sua inst√¢ncia EC2 e colar no navegador da seguinte forma `http://seu-dns-ipv4:8080`. Obtendo ent√£o a seguinte tela:

![Image 13](/assets/images/31-03-2025/Image 13.png)
_Login Apache Airflow_

### Criando o primeiro DAG

Vamos criar o primeiro DAG para teste do airflow, para isso crie um arquivo dentro da pasta `airflow/dags` utilizando o seguinte comando

```
nano my_first_dag.py
```

Copie e cole o seguinte c√≥digo

```python

from airflow import DAG
from airflow.operators.dummy_operator import DummyOperator
from datetime import datetime

default_args = {
    'owner': 'airflow',
    'start_date': datetime(2025, 1, 1),
    'retries': 1,
}

dag = DAG(
    'my_first_dag',
    default_args=default_args,
    schedule_interval='@daily',
)

start = DummyOperator(task_id='start', dag=dag)
end = DummyOperator(task_id='end', dag=dag)

start >> end

```

>No pr√≥ximo post, trataremos melhor sobre o assunto de DAG (Directed Acyclic Graph). Portanto, n√£o se apegue a este t√≥pico por enquanto.
{: .prompt-warning }

Abra a interface gr√°fica no navegador e fa√ßa o login utilizando as cred√™nciais criadas a pouco (No meu caso Login: airflow e Senha: airflow)

![Image 14](/assets/images/31-03-2025/Image 13.png)
_Login Apache Airflow_

Encontre a DAG criada `my_first_dag` e inicie a sua execu√ß√£o, se nenhum erro ocorrer voc√™ ver√° algo parecido com o retorno a seguir

![Image 15](/assets/images/31-03-2025/Image 14.png)
_DAG `my_first_dag` agendada sem nenhum erro_
