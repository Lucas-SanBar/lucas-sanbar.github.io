---
title: "Desbravando o Airflow - Parte 1 - Configurando ambiente e instalaÃ§Ã£o"
description: Aprenda a configuraÃ§Ã£o de um ambiente mÃ­nimo necessÃ¡rio para instalaÃ§Ã£o do Apache Airflow.
date: 2025-03-31
categories: [Pipeline de Dados, Apache Airflow]
tags: [airflow, dag, aws, ec2]
image: 
  path: /assets/images/31-03-2025/Capa-31-03-2025.png
---

## IntroduÃ§Ã£o

Com o crescimento exponencial dos dados, a necessidade de processÃ¡-los de forma eficiente nunca foi tÃ£o importante. Empresas e profissionais de dados enfrentam desafios constantes para extrair informaÃ§Ãµes a partir de grandes volumes de dados armazenados em bancos relacionais, planilhas e outros formatos.

Neste (E nos prÃ³ximos) artigo, vamos explorar a construÃ§Ã£o de um pipeline de dados, comeÃ§ando pela configuraÃ§Ã£o de uma instÃ¢ncia ec2 e instalaÃ§Ã£o do Apache Airflow.

Vamos entender como essa ferramenta se complementa com outras para criaÃ§Ã£o um fluxo de dados robusto e para garantir que tudo funcione de forma automÃ¡tica e escalÃ¡vel, utilizaremos o Apache **Airflow** para orquestrar o pipeline, agendar execuÃ§Ãµes e monitorar possÃ­veis falhas.

Ao final destes posts, vocÃª terÃ¡ um pipeline funcional que pode ser adaptado para diversos cenÃ¡rios, seja para processar dados de vendas, logs de sistemas ou qualquer outro conjunto de informaÃ§Ãµes estruturadas.

## O que Ã© o Apache Airflow?

O Apache **Airflow** Ã© uma ferramenta de cÃ³digo aberto para orquestraÃ§Ã£o de fluxos de trabalho. Ele permite criar, agendar e monitorar pipelines de dados de maneira programÃ¡vel e escalÃ¡vel. O **Airflow** Ã© amplamente utilizado para automaÃ§Ã£o de processos ETL, movimentaÃ§Ã£o de dados entre sistemas e integraÃ§Ã£o de ferramentas em um fluxo unificado.

![Intro 1](/assets/images/31-03-2025/IntroduÃ§Ã£o 1.png)
_`Apache Airflow` Logo_

### Principais caracterÃ­sticas

1. Baseado em DAGs (Directed Acyclic Graphs) â€“ Fluxos de trabalho sÃ£o representados como grafos direcionais acÃ­clicos, onde cada nÃ³ representa uma tarefa/atividade a ser executada.
2. Possui interface amigÃ¡vel e permite agendar tarefas e visualizar execuÃ§Ãµes via interface web.
3. Suporte a operadores personalizados e integraÃ§Ã£o com diversos serviÃ§os como AWS, Google Cloud, Databricks e outros.
4. Pode ser escalado horizontalmente, distribuindo tarefas entre mÃºltiplos workers.
5. Tarefas e pipelines sÃ£o definidos em cÃ³digo Python, garantindo flexibilidade e controle.

![Intro 2](/assets/images/31-03-2025/IntroduÃ§Ã£o 2.png)
_`Apache Airflow` Model_

### Casos de uso

1. **ETL/ELT** â€“ ExtraÃ§Ã£o, transformaÃ§Ã£o e carga de dados.
2. **OrquestraÃ§Ã£o de Machine Learning** â€“ AutomaÃ§Ã£o de pipelines de treinamento e inferÃªncia.
3. **IntegraÃ§Ã£o de Dados** â€“ MovimentaÃ§Ã£o de dados entre bancos SQL, NoSQL, APIs e sistemas de armazenamento.
4. **AutomaÃ§Ã£o de Processos** â€“ Gatilhos e dependÃªncias entre sistemas empresariais.

## O que Ã© o AWS EC2?

O AWS EC2 (Elastic Compute Cloud) Ã© um serviÃ§o da Amazon Web Services (AWS) que fornece mÃ¡quinas virtuais na nuvem. Ele permite que vocÃª crie e gerencie servidores virtuais (chamados de instÃ¢ncias) sob demanda, pagando apenas pelo tempo de uso.

![Intro 3](/assets/images/31-03-2025/IntroduÃ§Ã£o 3.png)
_`Amazon EC2` Logo_

### Principais caracterÃ­sticas

1. **Escalabilidade** â€“ VocÃª pode aumentar ou diminuir a capacidade conforme necessÃ¡rio.
2. **Modelos de InstÃ¢ncia** â€“ Oferece instÃ¢ncias otimizadas para diferentes cargas de trabalho (CPU, memÃ³ria, armazenamento, GPU, etc.).
3. **OpÃ§Ãµes de PreÃ§o** â€“ Inclui On-Demand (pagamento por hora/segundo), Reserved Instances (contratos de longo prazo mais baratos), Spot Instances (descontos em servidores ociosos) e Savings Plans.
4. **SeguranÃ§a** â€“ IntegraÃ§Ã£o com VPC (Virtual Private Cloud), IAM (Identity and Access Management) e outras ferramentas de seguranÃ§a.
5. **Flexibilidade** de SO â€“ Suporte a Linux, Windows e outros sistemas operacionais.

![Intro 4](/assets/images/31-03-2025/IntroduÃ§Ã£o 4.png)
_Sistemas operacionais suportados_

### Casos de uso

1. Hospedagem de aplicaÃ§Ãµes e sites.
2. ExecuÃ§Ã£o de bancos de dados e sistemas distribuÃ­dos.
3. Treinamento de modelos de Machine Learning.
4. SimulaÃ§Ãµes e processamento de dados intensivos.
5. Desenvolvimento e testes de software.

## Apache Airflow + AWS EC2

Usar **AWS EC2** para hospedar o **Apache Airflow** Ã© uma abordagem comum para quem deseja ter controle total sobre a configuraÃ§Ã£o do Airflow, sem depender de soluÃ§Ãµes gerenciadas mais robustas.

Essa abordagem funciona bem para projetos **pequenos e mÃ©dios**. Para workloads mais complexos, vocÃª pode considerar o MWAA (Managed Workflows for Apache Airflow) para evitar gerenciar a infraestrutura manualmente.

### Arquitetura BÃ¡sica

![Intro 5](/assets/images/31-03-2025/IntroduÃ§Ã£o 5.png)
_Apache Airflow + AWS EC2_

Para esta etapa utilizaremos a seguinte arquitetura e seguiremos os seguintes passos:

1. **InstÃ¢ncia EC2** â€“ CriaÃ§Ã£o e configuraÃ§Ã£o de uma instÃ¢ncia EC2 (Ubunto Pro 22.04).
2. **InstalaÃ§Ã£o Apache Airflow** â€“ InstalaÃ§Ã£o e configuraÃ§Ã£o do Apache Airflow na instÃ¢ncia criada na etapa anterior.
3. **Metadados** â€“ O Airflow precisa de um banco relacional para armazenar metadados (Usaremos PostgreSQL).
4. **Executores** â€“ ConfiguraÃ§Ã£o do executor local.
5. **Scheduler e Webserver** â€“ O Airflow precisa de um agendador para gerenciar tarefas e de um webserver para a interface grÃ¡fica.

## Criando a instÃ¢ncia EC2

Vamos passar pelo passo a passo para criar uma instÃ¢ncia EC2, configurar suas principais definiÃ§Ãµes e acessÃ¡-la remotamente.

### 1. Acessando o serviÃ§o EC2

ApÃ³s acessar o console da AWS, caso seja a sua primeira vez fazendo a visita busque por `EC2` na caixa de pesquisa do console.

![EC2 1](/assets/images/31-03-2025/Image EC2 1.png)
_Caixa de pesquisa do console AWS_

Caso nÃ£o seja a sua primeira vez visitando as instÃ¢ncias da AWS, vocÃª pode encontrar o serviÃ§o em **Visitado Recentemente**

![EC2 2](/assets/images/31-03-2025/Image EC2 2.png)
_Visitado Recentemente do console AWS_

No painel da Amazon EC2, o primeiro passo Ã© executar uma instÃ¢ncia, selecionando o seguinte botÃ£o na home page do painel

![EC2 3](/assets/images/31-03-2025/Image EC2 3.png)
_Painel Amazon EC2_

### 2. Configurando a instÃ¢ncia

![Image 1](/assets/images/31-03-2025/Image 1.png)
_ConfiguraÃ§Ãµes iniciais_

**1.** Defina um nome para sua instÃ¢ncia. No meu caso a instÃ¢ncia serÃ¡ "Airflow_Server".

**2.** Defina o Sistema Operacional a ser utilizado na sua instÃ¢ncia. No nosso caso selecione o **Ubunto Pro**.

>A melhor opÃ§Ã£o atualmente Ã© o Ubuntu Pro, por conter compatibilidade total com o Apache Airflow, Python, Docker e PostgreSQL. Para mais informaÃ§Ãµes acesse a documentaÃ§Ã£o do Apache Airflow pelo seguinte [ğŸ”—Link](https://airflow.apache.org/docs/apache-airflow/stable/installation/prerequisites.html).
{: .prompt-warning }

**3.** Definina a imagem do Sistema Operacional para 22.04 LTS.

>A melhor opÃ§Ã£o de imagem atualmete Ã© a 22.04 LTS, por conter maior suporte e estabilidade. A imagem 24.04 por ser um lanÃ§amento mais recente pode conter instabilidades.
{: .prompt-warning }

![Image 2](/assets/images/31-03-2025/Image 2.png)
_Tipo da instÃ¢ncia_

**4.** Selecione o tipo da instÃ¢ncia para T2.Small ou maior.

>Para uma boa utilizaÃ§Ã£o do Apache Airflow na sua instÃ¢ncia o aconselhado Ã© no mÃ­nimo 2GiB de memÃ³ria. Fique atento, qualquer tipo maior ou igual ao T2.Small pode ser **COBRADO**, para mais inforamÃ§Ã£o acesse a calculadora de preÃ§os da AWS no seguinte [ğŸ”—Link](https://calculator.aws/#/).
{: .prompt-warning }

![Image 3](/assets/images/31-03-2025/Image 3.png)
_Par de chaves_

**5.** Crie um novo par de chaves para acesso a sua instÃ¢ncia.
  - Selecione o tipo de par de chave como `RSA`;
  - Selecione o formato da chave como `.pem`;
  - Clique em `Criar par de chaves` ;

![Image 4](/assets/images/31-03-2025/Image 4.png)
_ConfiguraÃ§Ã£o da rede_

**6.** Para o nosso uso, ative:
  - PermissÃ£o para `SSH`;
  - PermissÃ£o para `HTTPs`;
  - PermissÃ£o para `HTTP`;

>Para o nosso objetivo/case de uso essa configuraÃ§Ã£o jÃ¡ Ã© o suficiente. Mas saiba que existe outros meios de acesso mais seguros para produÃ§Ã£o, para mais informaÃ§Ãµes acesse a documentaÃ§Ã£o do Apache Airflow pelo seguinte [ğŸ”—Link](https://airflow.apache.org/docs/apache-airflow/stable/installation/prerequisites.html).
{: .prompt-warning }

![Image 4.1](/assets/images/31-03-2025/Image 4-1.png)
_Resumo InstÃ¢ncia_

**7.** Confira o resumo da instÃ¢ncia a ser criada e caso tudo esteja conforme seu desejo selecione `Executar instÃ¢ncia`.

![Image 5](/assets/images/31-03-2025/Image 5.png)
_InstÃ¢ncia criada_

**8.** Aguarde a criaÃ§Ã£o da instÃ¢ncia.

![Image 6](/assets/images/31-03-2025/Image 9.png)
_Aba SeguranÃ§a_

![Image 7](/assets/images/31-03-2025/Image 10.png)
_Editar regras_

![Image 8](/assets/images/31-03-2025/Image 11.png)
_Criar nova regra_

**9.** Selecione a aba `SeguranÃ§a` da instÃ¢ncia criada.
  - VÃ¡ em `Editar regras de entrada`;
  - VÃ¡ em `Adicionar regra`;
  - Crie uma regra com as seguintes especificaÃ§Ãµes (TCP Personalizado e porta 8080);

![Image 9](/assets/images/31-03-2025/Image 12.png)
_Regra criada_

**10.** Pronto, a instÃ¢ncia foi configugurada e criada. Para acessar, basta selecionar a instÃ¢ncia e ir em `Conectar`.


## Instalando o Apache Airflow

![Image EC2 4](/assets/images/31-03-2025/Image EC2 4.png)
_Acesso_

ApÃ³s acessar a instÃ¢ncia criada nos passos anteriores, rode os seguintes comandos na ordem.

**1.** Atualize a lista de pacotes disponÃ­veis.
```
sudo apt update
```

**2.** Instale o gerenciador de pacotes `pip` para python 3.
```
sudo apt install python3-pip
```

**3.** Instale o SQLite3.
```
sudo apt install sqlite3
```

**4.** Instale o mÃ³dulo `venv` para criar ambientes virtuais no Python `3.10`.
```
sudo apt install python3.10-venv
```

**5.** Crie um ambiente virtual chamao `venv` (Ou qualquer outro nome que desejar) para isolar as dependÃªncias no Python.
```
python3 -m venv venv
```

**6.** Ativa o ambiente virtual criado no passo anterior `venv`, permitindo o uso de pacotes intalados dentro dele.
```
source venv/bin/activate
```

**7.** Instale a biblioteca de desenvolvimento do PostgreSQL (Opcional se nÃ£o for utilizar PostgreSQL como banco para os metadados, necessÃ¡rio para compilar extensÃµes que interagem com o banco de dados PostgreSQL).
```
sudo apt-get install libpq-dev
```

**8.** Instala o Apache Airflow na versÃ£o `2.5.0` com suporte ao PostgreSQL, garantindo compatibilidade com as dependÃªncias especificadas no arquivo para Python `3.7`.
```
pip install "apache-airflow[postgres]==2.5.0" --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-2.5.0/constraints-3.7.txt"
```

>Outras dependÃªncias podem ser utilizadas, verificar instalaÃ§Ã£o pelo [ğŸ”—Link](https://airflow.apache.org/docs/apache-airflow/stable/installation/installing-from-pypi.html).
{: .prompt-warning }

**9.** Inicialize o banco de daddos do Apache Airflow, criando as tabelas necessÃ¡rias para o funcionamento do sistema.
```
airflow db init
```

**10.**  Instale o PostgreSQL e pacotes adicionais com funcionalidades extras para o banco de dados.
```
sudo apt-get install postgresql postgresql-contrib
```

**11.** Mude para o usuÃ¡rio do sistema postgres, permitindo executar comandos administrativos no PostgreSQL.
```
sudo -i -u postgres
```

**12.** Abra o terminal interativo do PostgreSQL.
```
psql
```

**13.** Crie um banco de dados e um usuÃ¡rio para o Airflow no PostgreSQL e garanta todos os privilÃ©gios sobre o banco criado.
```
CREATE DATABASE airflow;
CREATE USER airflow WITH PASSWORD 'airflow';
GRANT ALL PRIVILEGES ON DATABASE airflow TO airflow;
```

**14.** Saia do terminal do PosgreSQL utilizando `Ctrl + D` e `Ctrl + D` (2x).

**15.** Entre no diretÃ³rio criado `airflow`.
```
cd airflow
```

**16.** Substitua a string de conexÃ£o no arquivo `airflow.cfg` por PostgreSQL ao invÃ©s de SQLite.
```
sed -i 's#sqlite:////home/ubuntu/airflow/airflow.db#postgresql+psycopg2://airflow:airflow@localhost/airflow#g' airflow.cfg
```

**17.** Verifique se a string de conexÃ£o foi alterada.
```
grep sql_alchemy airflow.cfg
```

![Image 10](/assets/images/31-03-2025/Image 6.png)
_Verifique se a conexÃ£o foi alterada para PostgreSQL_

**18.** Verifique se o tipo de executor do Airflow Ã© do tipo `SequentialExecutor`.
```
grep executor airflow.cfg
```

![Image 11](/assets/images/31-03-2025/Image 7.png)
_Verifique o tipo do executor_

**19.** Substitua o mÃ©todo do executor para `LocalExecutor`.
```
sed -i 's#SequentialExecutor#LocalExecutor#g' airflow.cfg
```

**20.** Verifique se o tipo de executor foi alterado.
```
grep executor airflow.cfg
```

![Image 12](/assets/images/31-03-2025/Image 8.png)
_Verifique o tipo do executor foi alterado_

**21.** Crie o diretÃ³rio para armazenamento dos DAGS.
```
mkdir dags
```

**22.** Inicie novamente o banco de dados do Airflow.
```
airflow db init
```

**23.** Crie um usuÃ¡rio do Airflow.
  - Login: `airflow`
  - Senha: `airflow`

```
airflow users create -u airflow -f airflow -l airflow -r Admin -e airflow@gmail.com
```

**24.** Inicie o servidor web para interface grÃ¡fica.
```
airflow webserver &
```

**25.** Inicie o agendador do Airflow.
```
airflow scheduler
```

Se na execuÃ§Ã£o de todos os comandos nenhum erro foi retornado, o apache foi instalado e configurado para acesso via navegador. Basta copiar o IPv4 DNS da sua instÃ¢ncia EC2 e colar no navegador da seguinte forma `http://seu-dns-ipv4:8080`. Obtendo entÃ£o a seguinte tela:

![Image 13](/assets/images/31-03-2025/Image 13.png)
_Login Apache Airflow_

## Criando o primeiro DAG

Vamos criar o primeiro DAG para teste do airflow, para isso crie um arquivo dentro da pasta `airflow/dags` utilizando o seguinte comando

```
nano my_first_dag.py
```

Copie e cole o seguinte cÃ³digo

```python

from airflow import DAG
from airflow.operators.dummy_operator import DummyOperator
from datetime import datetime

default_args = {
    'owner': 'airflow',
    'start_date': datetime(2025, 1, 1),
    'retries': 1,
}

dag = DAG(
    'my_first_dag',
    default_args=default_args,
    schedule_interval='@daily',
)

start = DummyOperator(task_id='start', dag=dag)
end = DummyOperator(task_id='end', dag=dag)

start >> end

```

>No prÃ³ximo post, trataremos melhor sobre o assunto de DAG (Directed Acyclic Graph). Portanto, nÃ£o se apegue a este tÃ³pico por enquanto.
{: .prompt-warning }

Abra a interface grÃ¡fica no navegador e faÃ§a o login utilizando as credÃªnciais criadas a pouco (No meu caso Login: airflow e Senha: airflow)

![Image 14](/assets/images/31-03-2025/Image 13.png)
_Login Apache Airflow_

Encontre a DAG criada `my_first_dag` e inicie a sua execuÃ§Ã£o, se nenhum erro ocorrer vocÃª verÃ¡ algo parecido com o retorno a seguir:

![Image 15](/assets/images/31-03-2025/Image 14.png)
_DAG `my_first_dag` agendada sem nenhum erro_

ParabÃ©ns! ğŸ‰ğŸ‰ğŸ‰ Agora que seu primeiro DAG foi criado no Apache Airflow, vocÃª deu o primeiro passo para automatizar fluxos de dados e orquestrar tarefas de forma mais eficiente.

Com essa estrutura inicial, vocÃª pode:
1. Executar e monitorar tarefas diretamente pela interface web do Airflow.
2. Adicionar dependÃªncias e agendamentos, tornando seu fluxo mais dinÃ¢mico.
3. Integrar com bancos de dados, APIs e outras ferramentas para processar dados de forma escalÃ¡vel.

Nosso prÃ³ximo passo serÃ¡ entender mais sobre a estrutura do DAG, operadores e conexÃµes para tornar seus pipelines mais robustos e automatizados.
